name: Looker Studio Scraper

on:
  workflow_dispatch: {}
  schedule:
    # 08:05, 11:05, 14:05, 17:05, 20:05 London (cron is UTC)
    - cron: "5 7,10,13,16,19 * * *"

concurrency:
  group: looker-scrape
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install playwright requests schedule
          python -m playwright install --with-deps chromium

      # ========= RESTORE STATE (CACHE FIRST) =========
      - name: Restore scraper state (auth_state.json + comments_log.csv) from cache
        id: cache-restore
        uses: actions/cache/restore@v4
        with:
          path: |
            auth_state.json
            comments_log.csv
          # stable key so we can restore exact versions on future runs
          key: scraper-state-${{ runner.os }}-${{ hashFiles('scrape.py') }}
          restore-keys: |
            scraper-state-${{ runner.os }}-
            scraper-state-

      # Optional fallback: if cache didn’t have auth_state.json, use the secret
      - name: Fallback restore auth_state.json from secret
        if: ${{ !hashFiles('auth_state.json') }}
        env:
          AUTH_STATE_B64: ${{ secrets.AUTH_STATE_B64 }}
        run: |
          if [ -n "$AUTH_STATE_B64" ]; then
            echo "$AUTH_STATE_B64" | base64 -d > auth_state.json
            echo "Restored auth_state.json from AUTH_STATE_B64 secret."
          else
            echo "No AUTH_STATE_B64 secret provided — continuing without auth_state.json."
          fi

      # ========= CONFIG =========
      - name: Create/Update config.ini from secrets
        run: |
          {
            echo "[DEFAULT]"
            echo "GOOGLE_EMAIL=${{ secrets.GOOGLE_EMAIL }}"
            echo "GOOGLE_PASSWORD=${{ secrets.GOOGLE_PASSWORD }}"
            echo "MAIN_WEBHOOK=${{ secrets.MAIN_WEBHOOK }}"
            echo "ALERT_WEBHOOK=${{ secrets.ALERT_WEBHOOK }}"
          } > config.ini

      # ========= RUN =========
      - name: Run scraper (headed via Xvfb, phone 2FA supported)
        env:
          CI_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          xvfb-run -a python scrape.py now

      # ========= SAVE UPDATED STATE (CACHE) =========
      # Save each file only if it exists to avoid "Path Validation Error"
      - name: Save auth_state.json to cache (if exists)
        if: ${{ hashFiles('auth_state.json') != '' }}
        uses: actions/cache/save@v4
        with:
          path: auth_state.json
          key: scraper-state-${{ runner.os }}-${{ hashFiles('scrape.py') }}

      - name: Save comments_log.csv to cache (if exists)
        if: ${{ hashFiles('comments_log.csv') != '' }}
        uses: actions/cache/save@v4
        with:
          path: comments_log.csv
          key: scraper-state-${{ runner.os }}-${{ hashFiles('scrape.py') }}

      # ========= OPTIONAL: ARTIFACTS FOR INSPECTION =========
      - name: Upload current state as artifact (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-state
          path: |
            auth_state.json
            comments_log.csv
          if-no-files-found: ignore

      - name: Upload logs & screenshots (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-and-screens
          path: |
            scrape.log
            screens/*
          if-no-files-found: ignore
