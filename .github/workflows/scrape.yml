name: Looker Studio Scraper Suite

on:
  workflow_dispatch: {}
  schedule:
    # 1. Daily Report at 8:30 AM GMT (8:30 UTC)
    - cron: "30 8 * * *"
    # 2. Hourly Updates at 11:00, 13:00, 15:00, 17:00, 19:00, 21:00 GMT (11:00 to 21:00 UTC, every 2 hours)
    - cron: "0 11,13,15,17,19,21 * * *"

concurrency:
  group: looker-scrape
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 40

    steps:
      # ────────────────────────────────
      # Checkout
      # ────────────────────────────────
      - name: Checkout
        uses: actions/checkout@v4

      # ────────────────────────────────
      # Setup Python + Playwright + Gemini Libs (No Tesseract needed)
      # ────────────────────────────────
      - name: Setup Python & Install Dependencies
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      
      - name: Install Python deps (Playwright, Gemini, etc.)
        run: |
          python -m pip install -U pip
          # Removed pytesseract. Added google-genai.
          pip install playwright requests pillow google-genai schedule pytz
          python -m playwright install --with-deps chromium

      # ────────────────────────────────
      # Export helpers/env
      # ────────────────────────────────
      - name: Export TODAY, CI_RUN_URL, ROI_MAP_FILE, and GEMINI_API_KEY
        run: |
          echo "TODAY=$(date -u +%Y%m%d)" >> $GITHUB_ENV
          echo "CI_RUN_URL=${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_ENV
          echo "ROI_MAP_FILE=${{ github.workspace }}/roi_map.json" >> $GITHUB_ENV
          echo "GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}" >> $GITHUB_ENV

      # ────────────────────────────────
      # Restore state from cache (auth + CSV logs + ROI map)
      # ────────────────────────────────
      - name: Restore scraper state (auth + logs + ROI)
        id: cache-restore
        uses: actions/cache/restore@v4
        with:
          path: |
            auth_state.json
            comments_log.csv
            complaints_log.csv
            daily_report_log.csv
            roi_map.json
          key: scraper-state-${{ runner.os }}-v1-${{ github.ref_name }}-${{ env.TODAY }}
          restore-keys: |
            scraper-state-${{ runner.os }}-v1-${{ github.ref_name }}-
            scraper-state-${{ runner.os }}-v1-

      # ────────────────────────────────
      # Optional fallback (first-ever run)
      # ────────────────────────────────
      - name: Fallback restore auth_state.json from secret
        run: |
          if [ -n "${AUTH_STATE_B64}" ]; then
            echo "${AUTH_STATE_B64}" | base64 --decode > auth_state.json
            echo "Decoded auth_state.json from AUTH_STATE_B64."
          else
            echo "No AUTH_STATE_B64 secret provided — continuing without."
          fi
        env:
          AUTH_STATE_B64: ${{ secrets.AUTH_STATE_B64 }}

      # ────────────────────────────────
      # Create config.ini from secrets
      # ────────────────────────────────
      - name: Create/Update config.ini from secrets
        run: |
          {
            echo "[DEFAULT]"
            echo "GOOGLE_EMAIL=${{ secrets.GOOGLE_EMAIL }}"
            echo "GOOGLE_PASSWORD=${{ secrets.GOOGLE_PASSWORD }}"
            echo "MAIN_WEBHOOK=${{ secrets.MAIN_WEBHOOK }}"
            echo "ALERT_WEBHOOK=${{ secrets.ALERT_WEBHOOK }}"
            echo "COMPLAINTS_WEBHOOK=${{ secrets.COMPLAINTS_WEBHOOK }}"
            echo "DAILY_WEBHOOK=${{ secrets.DAILY_WEBHOOK }}"
            echo "GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}"
          } > config.ini

      # ────────────────────────────────
      # Run scrapers (Logic for Daily Report at 8:30 AM UTC)
      # ────────────────────────────────
      - name: Determine if Full or Partial Run
        id: determine_run_type
        run: |
          # Check if the trigger is the 8:30 AM UTC daily report (full run)
          IS_DAILY_REPORT="false"
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            IS_DAILY_REPORT="true" # Always run full on manual dispatch
          elif [[ "${{ github.event.schedule }}" == "30 8 * * *" ]]; then
            IS_DAILY_REPORT="true"
          fi
          echo "IS_DAILY_REPORT=$IS_DAILY_REPORT" >> $GITHUB_OUTPUT
      
      - name: Run NPS scraper (All Runs)
        run: xvfb-run -a python scrape.py now || true

      - name: Run Complaints scraper (All Runs)
        run: xvfb-run -a python scrape_complaints.py now || true

      - name: Run Daily Report scraper (Only on Daily Report Schedule)
        if: steps.determine_run_type.outputs.IS_DAILY_REPORT == 'true'
        run: xvfb-run -a python scrape_daily.py || true

      # ────────────────────────────────
      # Show file states for debugging
      # ────────────────────────────────
      - name: Show state files
        run: |
          echo "::group::State files"
          for f in auth_state.json comments_log.csv complaints_log.csv daily_report_log.csv roi_map.json scrape.log scrape_complaints.log scrape_daily.log; do
            if [ -f "$f" ]; then
              echo "[OK] $f -> $(wc -c < "$f") bytes"
            else
              echo "[MISS] $f"
            fi
          done
          echo "::endgroup::"

          echo "::group::daily_report_log.csv (last 5)"
          if [ -f daily_report_log.csv ]; then tail -n 5 daily_report_log.csv || true; else echo "daily_report_log.csv not found"; fi
          echo "::endgroup::"

          echo "::group::screens/ listing"
          if [ -d screens ]; then ls -lah screens || true; else echo "no screens/"; fi
          echo "::endgroup::"

      # ────────────────────────────────
      # Upload artifacts for inspection
      # ────────────────────────────────
      - name: Upload logs and state as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-state-${{ github.run_id }}
          path: |
            auth_state.json
            comments_log.csv
            complaints_log.csv
            daily_report_log.csv
            roi_map.json
            scrape.log
            scrape_complaints.log
            scrape_daily.log
            screens/
          if-no-files-found: ignore

      # ────────────────────────────────
      # Save updated cache snapshot
      # ────────────────────────────────
      - name: Save updated scraper state to cache
        uses: actions/cache/save@v4
        with:
          path: |
            auth_state.json
            comments_log.csv
            complaints_log.csv
            daily_report_log.csv
            roi_map.json
          key: scraper-state-${{ runner.os }}-v1-${{ github.run_id }}
