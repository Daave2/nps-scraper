# üìñ Looker Studio Scraper Suite: Full Technical Manual

This repository contains a multi-script, scheduled scraping solution designed to extract business-critical data from Google Looker Studio reports and deliver formatted, actionable alerts to Google Chat. It leverages headless browsing (Playwright) and AI-powered image analysis (Gemini Vision) to ensure robust data capture.

---

## 1. Overview and Architecture

The suite is composed of specialized Python scripts, each with a distinct target and logic, orchestrated by a GitHub Actions workflow. State and logs are managed through shared files, which are cached between runs to maintain continuity and prevent data duplication.

### File Manifest

| File | Role | Execution | Logic Focus |
| :--- | :--- | :--- | :--- |
| **`scrape_daily.py`** | **Daily Summary Report Generator** | Daily (8:30 AM UTC) | Hybrid text-parsing and Gemini Vision for metrics, screenshot capture, CSV logging. |
| **`scrape.py`** | **NPS Comment Scraper** | Multiple Times Daily | Interactive 2FA handling, text parsing for comments, batched sending, file-based locking. |
| **`scrape_complaints.py`** | **Customer Complaints Scraper** | Multiple Times Daily | State-machine text parsing, case number deduplication, per-complaint card alerting. |
| **`.github/workflows/...`** | **GitHub Actions Workflow** | Scheduled / Manual | Scheduling, environment setup, state caching, conditional script execution, artifact management. |
| **`auth_state.json`** | **Shared Resource** | Stores the authenticated browser session (cookies, local storage) as a JSON object, enabling non-interactive logins. |
| **`comments_log.csv`** | **Shared Resource** | Logs unique NPS comments (`store`, `timestamp`, `comment`) to prevent resending. |
| **`complaints_log.csv`**| **Shared Resource** | Logs unique complaint `case_number`s to prevent duplicate alerts. |
| **`daily_report_log.csv`**| **Shared Resource** | Logs a daily snapshot of all metrics extracted by `scrape_daily.py`. |

---

## 2. Configuration: GitHub Secrets

The system is configured entirely via **GitHub Secrets**, ensuring that sensitive credentials are not hard-coded. These secrets are injected into the runtime environment by the GitHub Actions workflow and written to a temporary `config.ini` file for the scripts to consume.

| Secret Name | Purpose & Script Usage |
| :--- | :--- |
| `GOOGLE_EMAIL` | The Google account email for accessing Looker Studio reports. Used by all scripts. |
| `GOOGLE_PASSWORD` | The password for the Google account. Used only for the initial or re-login flow in `scrape.py`. |
| `GEMINI_API_KEY` | **Critical for `scrape_daily.py`**. Provides the authentication token for the Google Gemini API. |
| `MAIN_WEBHOOK` | Target Google Chat webhook for batched NPS comments from `scrape.py`. |
| `ALERT_WEBHOOK` | Target webhook for high-priority alerts: login failures, 2FA codes, and critical script errors from all scripts. |
| `DAILY_WEBHOOK` | Target webhook for the comprehensive daily summary card generated by `scrape_daily.py`. |
| `COMPLAINTS_WEBHOOK`| Target webhook for individual new complaint notifications from `scrape_complaints.py`. |
| `AUTH_STATE_B64` | **(Optional)** A Base64-encoded string of a valid `auth_state.json` file. Used for disaster recovery or to migrate an authenticated session to a new repository without re-doing the 2FA process. |

---

## 3. Manual Operational Guide: Initial Login & 2FA

**The first successful run is a required manual process** to handle Google's 2-Factor Authentication (2FA) and create the shared `auth_state.json` file. This file is the key to all subsequent automated runs.

### Initial Authentication Procedure

1.  **Set Secrets:** Navigate to `Settings` > `Secrets and variables` > `Actions` in the GitHub repository. Ensure all required secrets (especially `GOOGLE_EMAIL`, `GOOGLE_PASSWORD`, and `ALERT_WEBHOOK`) are correctly set.
2.  **Trigger Login:** Go to the **Actions** tab, select the **Looker Studio Scraper Suite** workflow from the sidebar, and click the **Run workflow** button.
3.  **Wait for Alert:** The `scrape.py` process will detect the need for 2FA and pause. The workflow job will remain in a "running" state.
4.  **2FA Intervention (Critical Step):**
    *   The script's `wait_for_2fa_and_alert` function will analyze the login screen to find the verification number.
    *   It will send an alert to the configured **`ALERT_WEBHOOK`**. The message will look like this: `üîê Tap this number on your phone: **42**`.
    *   **Action Required:** You must immediately approve the sign-in request on your designated mobile device and tap the number that matches the one in the alert. You have a limited time (approx. 3 minutes) before the script times out.
5.  **Completion & Verification:** Once approved, the script will complete the login, save the authenticated session to `auth_state.json`, and the workflow will cache this file. All future scheduled runs will restore this file from the cache, bypassing the need for manual login.

---

## 4. Deep Dive: Python Script Logic

### A. `scrape_daily.py` (Retail Daily Summary)

This script uses a hybrid approach, combining fast layout-based parsing with AI-powered visual analysis for maximum accuracy.

| Feature | Logic Mechanism |
| :--- | :--- |
| **Hybrid Parsing Strategy** | The script first attempts to extract all metrics using `parse_from_lines`, a function that relies on positional logic and regular expressions based on the report's text layout. This provides a fast first pass for well-structured data. |
| **Gemini Vision Fallback**| After the initial parse, `extract_gemini_metrics` identifies metrics that still have a placeholder value (`"‚Äî"`) or are on the `GEMINI_METRICS` list for mandatory AI validation. It sends a full-page screenshot to the **Gemini Vision** API with a structured prompt and a defined JSON response schema, ensuring accurate extraction of visually-encoded data like NPS dials, charts, and percentages. |
| **Data Stabilization** | The `open_and_prepare` function enforces a `page.wait_for_timeout(12_000)` after the initial page load. This critical pause allows all dynamic, JavaScript-rendered content and data visualizations to fully load and stabilize before text extraction or screenshotting occurs, preventing race conditions and incomplete data capture. |

### B. `scrape_complaints.py` (Customer Complaints)

Engineered to parse semi-structured, multi-line text into coherent records and deliver immediate, individual alerts.

| Feature | Logic Mechanism |
| :--- | :--- |
| **State-Machine Parsing** | The core parsing logic in `parse_complaints_from_lines` is a state machine. It iterates through each line of text, transitioning between states (e.g., `LOOKING_FOR_START`, `FOUND_DATE`, `READING_DESC`) based on regex matches. This allows it to reliably group related but physically separate lines of text into a single, coherent complaint record. |
| **Case Deduplication** | `read_existing_complaints` loads all historical `case_number`s from `complaints_log.csv` into a Python `set` for highly efficient lookup (`O(1)` average time complexity). Only new cases not present in this set are processed, sent to Google Chat, and then appended to the log. |
| **Per-Complaint Alerting** | Each new complaint is formatted into a distinct Google Chat card and sent individually via `send_complaint_to_google_chat`. This ensures immediate visibility for each new case, rather than batching them into a summary. |

### C. `scrape.py` (NPS Comments)

This script is built for resilience, especially around authentication, rate limiting, and preventing concurrent executions.

| Feature | Logic Mechanism |
| :--- | :--- |
| **2FA Extraction** | The `wait_for_2fa_and_alert` function uses targeted regex (`RE_TWO_OR_THREE`) and heuristic filtering to reliably extract the 2- or 3-digit verification code. It intelligently ignores common false positives, such as device model numbers (e.g., "14T", "13 Pro"), by pre-emptively removing them from the screen text before searching. |
| **Batched Sending** | New comments are grouped into chunks (`BATCH_SIZE=10`). The `send_comments_batched_to_chat` loop makes a separate POST request for each batch to respect Google Chat API rate limits. The underlying `_post_with_backoff` helper automatically handles `429 Too Many Requests` errors by sleeping and retrying with exponential backoff. |
| **Stale Lock Cleanup** | The `_acquire_lock` function checks the modification time of an existing `scrape.lock` file. If the file is older than `STALE_LOCK_MAX_AGE_S` (20 minutes), it is considered "stale" (from a failed run) and is automatically deleted. This self-healing mechanism prevents a single failure from blocking the entire schedule indefinitely. |

---

## 5. GitHub Actions Workflow (`Looker Studio Scraper Suite`)

The workflow is the backbone of the suite, managing scheduling, environment, state, and execution.

| Component | Logic & Purpose |
| :--- | :--- |
| **Triggers** | **Scheduled (Cron):** A "Daily Full Run" is scheduled for `30 8 * * *` (8:30 AM UTC), which runs all scripts. "Hourly Partial Runs" are scheduled for `0 11,13,15,17,19,21 * * *`, executing only the frequent NPS and complaints scrapers. **Manual (`workflow_dispatch`):** Allows for on-demand runs, which always execute the Full Run configuration. |
| **Setup & Env** | Installs Python, project dependencies (Playwright, requests, google-genai, etc.), and Playwright's browser binaries. It then exports secrets to a `config.ini` file and helper variables (like `CI_RUN_URL`) to the environment via `$GITHUB_ENV`. |
| **Caching** | Uses `actions/cache` to persist shared state files (`auth_state.json`, `*log.csv`, etc.) across workflow runs. This is the core mechanism that enables persistent login, as a valid session from a previous run is restored at the start of a new one. The cache key includes the date (`env.TODAY`) to ensure a fresh cache is created daily. |
| **Conditional Execution** | A `determine_run_type` step checks `github.event.schedule` and `github.event_name` to set the `IS_DAILY_REPORT` output variable. The step for executing `scrape_daily.py` is then guarded by the condition `if: steps.determine_run_type.outputs.IS_DAILY_REPORT == 'true'`. |
| **Execution Command** | All Python scripts are launched using the command `xvfb-run -a python...`. This is **mandatory** as `xvfb` provides a virtual framebuffer (an in-memory display), which Chromium requires for rendering, even when running in a headless Linux environment. |
| **Artifacts** | Upon completion (success or failure), the workflow gathers all state files, logs (`*.log`), and any debugging screenshots/HTML from the `screens/` directory and uploads them as a job artifact. This provides essential, downloadable evidence for debugging and operational review. |

---

## 6. Troubleshooting and FAQs

#### **Authentication & Login Failures**

**Q: The script is stuck in a login loop or constantly asks for 2FA on every run.**
**A:** This usually means the cached `auth_state.json` is invalid or has expired.
*   **Cause 1: Credentials Changed.** The Google account password may have been changed. Verify the `GOOGLE_EMAIL` and `GOOGLE_PASSWORD` secrets are correct.
*   **Cause 2: Session Expired.** Google may have invalidated the session for security reasons.
*   **Solution:**
    1.  Go to the **Actions** tab in GitHub.
    2.  Click on **Caches** in the left sidebar.
    3.  Find the cache key starting with `scraper-state-` and manually delete it.
    4.  Manually re-run the **Looker Studio Scraper Suite** workflow to trigger the 2FA process again and generate a fresh `auth_state.json`.

**Q: How do I force a re-login without changing secrets?**
**A:** Follow the "Solution" steps above to delete the GitHub Actions cache. This will force the next run to start without a saved `auth_state.json`, triggering the manual login flow.

#### **Data & Parsing Issues**

**Q: The daily report is missing data or a metric is incorrect.**
**A:** This can be a text-parsing issue or a Gemini Vision issue.
1.  **Check the Artifacts:** Find the failed workflow run, and download the `scraper-state` or `test-daily-report` artifact.
2.  **Inspect `screens/`:** Look at the `_fullpage.png` to see what the scraper saw. Is the data visible?
3.  **Inspect `_lines.txt`:** This file shows the raw text extracted from the page. If the data is missing or garbled here, the Looker Studio report may have changed its layout. The parsing logic in `scrape_daily.py` (e.g., the `parse_from_lines` function) may need to be updated.
4.  **Check Gemini Logs:** Review the `scrape_daily.log` file. If there are errors related to the Gemini API, ensure the `GEMINI_API_KEY` secret is correct and has not expired.

**Q: No new NPS comments or complaints are being posted, but I know there are new ones.**
**A:** This is likely a deduplication issue or a parsing failure.
1.  **Check the Artifacts:** Download the artifacts for the run.
2.  **Inspect the Logs:** Look at `scrape.log` or `scrape_complaints.log`. It will state `No new comments/complaints to send.` if it successfully filtered them. It will log errors if parsing failed.
3.  **Inspect the Text Dump:** Check the `screens/*_text.txt` file to see the raw text. If the report's layout has changed, the state-machine parser in `scrape_complaints.py` or the anchor-based parser in `scrape.py` might be failing to identify new entries.

#### **Workflow & Execution Failures**

**Q: The workflow failed with a "Timeout" error.**
**A:** The default timeout is 40 minutes. This can happen if the Looker Studio report is extremely slow to load or if the script is waiting for a 2FA approval that never comes. Re-run the job. If it persists, the report's performance should be investigated.

**Q: Where do I find the detailed logs and screenshots?**
**A:**
1.  Go to the **Actions** tab.
2.  Click on the specific workflow run that failed or you want to inspect.
3.  On the summary page for that run, scroll down to the **"Artifacts"** section.
4.  Download the `.zip` file (e.g., `scraper-state-12345`). It will contain all the log files, state files, and screenshots captured during that run.

**Q: I see "Stale lock detected" in the logs. What does this mean?**
**A:** This is a self-healing message, not an error. It means a previous run of `scrape.py` or `scrape_complaints.py` crashed or was terminated without properly removing its `.lock` file. The current script detected the old lock file, saw it was older than 20 minutes, and removed it to proceed. This is expected behavior for ensuring resilience.
