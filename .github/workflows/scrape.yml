name: Looker Studio Scraper Suite

on:
  workflow_dispatch: {}
  schedule:
    # Daily Report: Runs once at 8:30 AM GMT (08:30 UTC)
    - cron: "30 8 * * *" 
    # Periodic Scrapes: Runs every 2 hours from 10:30 AM to 8:30 PM GMT (10:30-20:30 UTC)
    - cron: "30 10-20/2 * * *"

concurrency:
  group: looker-scrape
  cancel-in-progress: true 

jobs:
  # ----------------------------------------------------------------------
  # Shared Setup (for all runs)
  # ----------------------------------------------------------------------
  setup_and_cache:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Setup Python & Install Dependencies (Gemini Ready)
        id: install_deps
        run: |
          sudo apt-get update
          # Install core packages + Gemini/Playwright
          python -m pip install -U pip
          pip install playwright requests pillow google-genai schedule pytz
          python -m playwright install --with-deps chromium

      - name: Export ENV VARS
        id: export_vars
        run: |
          echo "TODAY=$(date -u +%Y%m%d)" >> $GITHUB_ENV
          echo "CI_RUN_URL=${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_ENV
          echo "ROI_MAP_FILE=${{ github.workspace }}/roi_map.json" >> $GITHUB_ENV
          echo "GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}" >> $GITHUB_ENV

      - name: Restore scraper state (auth + logs)
        id: cache-restore
        uses: actions/cache/restore@v4
        with:
          path: |
            auth_state.json
            comments_log.csv
            complaints_log.csv
            daily_report_log.csv
            roi_map.json
          key: scraper-state-${{ runner.os }}-v1-${{ github.ref_name }}-${{ env.TODAY }}
          restore-keys: |
            scraper-state-${{ runner.os }}-v1-${{ github.ref_name }}-

      - name: Create config.ini from secrets
        id: create_config
        run: |
          {
            echo "[DEFAULT]"
            echo "GOOGLE_EMAIL=${{ secrets.GOOGLE_EMAIL }}"
            echo "GOOGLE_PASSWORD=${{ secrets.GOOGLE_PASSWORD }}"
            echo "MAIN_WEBHOOK=${{ secrets.MAIN_WEBHOOK }}"
            echo "ALERT_WEBHOOK=${{ secrets.ALERT_WEBHOOK }}"
            echo "COMPLAINTS_WEBHOOK=${{ secrets.COMPLAINTS_WEBHOOK }}"
            echo "DAILY_WEBHOOK=${{ secrets.DAILY_WEBHOOK }}"
            echo "GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}"
          } > config.ini

      - name: Save Cache Artifacts
        id: save_artifacts
        if: always()
        run: |
          # The next jobs will use the output files, so no need to save cache now.
          # The final cache save will be done after the scrape jobs run.
          echo "SETUP COMPLETE"


  # ----------------------------------------------------------------------
  # JOB 2: Daily Report (Runs once at 8:30 AM GMT)
  # ----------------------------------------------------------------------
  daily_report:
    runs-on: ubuntu-latest
    timeout-minutes: 40
    needs: setup_and_cache
    if: github.event_name == 'workflow_dispatch' || github.event.schedule == '30 8 * * *'

    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      # Use the same setup steps as before, but only for this job's needs (faster execution)
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Re-install Core Deps (minimal, already pulled by cache)
        run: pip install playwright requests pillow google-genai schedule pytz

      - name: Export ENV VARS (Required for script access)
        run: |
          echo "CI_RUN_URL=${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_ENV
          echo "GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}" >> $GITHUB_ENV

      # Restore state again (The full state is only restored/saved *here* for correct file persistence)
      - name: Restore scraper state for job (Auth/Logs)
        uses: actions/cache/restore@v4
        with:
          path: |
            auth_state.json
            comments_log.csv
            complaints_log.csv
            daily_report_log.csv
            roi_map.json
          key: scraper-state-${{ runner.os }}-v1-${{ github.ref_name }}-${{ env.TODAY }}
          restore-keys: |
            scraper-state-${{ runner.os }}-v1-${{ github.ref_name }}-

      - name: Create config.ini from secrets
        run: |
          {
            echo "[DEFAULT]"
            echo "GOOGLE_EMAIL=${{ secrets.GOOGLE_EMAIL }}"
            echo "GOOGLE_PASSWORD=${{ secrets.GOOGLE_PASSWORD }}"
            echo "MAIN_WEBHOOK=${{ secrets.MAIN_WEBHOOK }}"
            echo "ALERT_WEBHOOK=${{ secrets.ALERT_WEBHOOK }}"
            echo "COMPLAINTS_WEBHOOK=${{ secrets.COMPLAINTS_WEBHOOK }}"
            echo "DAILY_WEBHOOK=${{ secrets.DAILY_WEBHOOK }}"
            echo "GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}"
          } > config.ini

      # ────────────────────────────────
      # Run ALL 3 Scrapers
      # ────────────────────────────────
      - name: Run NPS scraper
        run: xvfb-run -a python scrape.py now || true

      - name: Run Complaints scraper
        run: xvfb-run -a python scrape_complaints.py now || true

      - name: Run Daily Report scraper (Gemini Vision)
        run: xvfb-run -a python scrape_daily.py || true

      # ────────────────────────────────
      # Upload/Save Cache (Final State Save)
      # ────────────────────────────────
      - name: Upload logs and state as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-state-daily-${{ github.run_id }}
          path: |
            auth_state.json
            comments_log.csv
            complaints_log.csv
            daily_report_log.csv
            roi_map.json
            scrape.log
            scrape_complaints.log
            scrape_daily.log
            screens/
          if-no-files-found: ignore

      - name: Save updated scraper state to cache
        uses: actions/cache/save@v4
        with:
          path: |
            auth_state.json
            comments_log.csv
            complaints_log.csv
            daily_report_log.csv
            roi_map.json
          key: scraper-state-${{ runner.os }}-v1-${{ github.run_id }}


  # ----------------------------------------------------------------------
  # JOB 3: Periodic Scrapes (Runs every 2 hours after the daily report)
  # ----------------------------------------------------------------------
  periodic_scrapes:
    runs-on: ubuntu-latest
    timeout-minutes: 40
    # Runs ONLY if the event is a schedule and the cron time does NOT match the daily report time
    if: github.event.schedule != '30 8 * * *'
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # ────────────────────────────────
      # Install Dependencies (Minimal for periodic runs)
      # ────────────────────────────────
      - name: Setup Python & Install Dependencies
        run: |
          sudo apt-get update
          python -m pip install -U pip
          pip install playwright requests pillow pytesseract schedule pytz
          python -m playwright install --with-deps chromium

      - name: Export ENV VARS
        run: |
          echo "CI_RUN_URL=${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_ENV

      # ────────────────────────────────
      # Restore State
      # ────────────────────────────────
      - name: Restore scraper state (auth + logs)
        uses: actions/cache/restore@v4
        with:
          path: |
            auth_state.json
            comments_log.csv
            complaints_log.csv
            daily_report_log.csv
            roi_map.json
          key: scraper-state-${{ runner.os }}-v1-${{ github.ref_name }}-${{ env.TODAY }}
          restore-keys: |
            scraper-state-${{ runner.os }}-v1-${{ github.ref_name }}-

      - name: Create config.ini (Standard)
        run: |
          {
            echo "[DEFAULT]"
            echo "GOOGLE_EMAIL=${{ secrets.GOOGLE_EMAIL }}"
            echo "GOOGLE_PASSWORD=${{ secrets.GOOGLE_PASSWORD }}"
            echo "MAIN_WEBHOOK=${{ secrets.MAIN_WEBHOOK }}"
            echo "ALERT_WEBHOOK=${{ secrets.ALERT_WEBHOOK }}"
            echo "COMPLAINTS_WEBHOOK=${{ secrets.COMPLAINTS_WEBHOOK }}"
            echo "DAILY_WEBHOOK=${{ secrets.DAILY_WEBHOOK }}"
          } > config.ini

      # ────────────────────────────────
      # Run ONLY NPS and Complaints Scrapers
      # ────────────────────────────────
      - name: Run NPS scraper
        run: xvfb-run -a python scrape.py now || true

      - name: Run Complaints scraper
        run: xvfb-run -a python scrape_complaints.py now || true

      # ────────────────────────────────
      # Upload/Save Cache
      # ────────────────────────────────
      - name: Upload logs and state as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-state-periodic-${{ github.run_id }}
          path: |
            auth_state.json
            comments_log.csv
            complaints_log.csv
            daily_report_log.csv
            roi_map.json
            scrape.log
            scrape_complaints.log
          if-no-files-found: ignore

      - name: Save updated scraper state to cache
        uses: actions/cache/save@v4
        with:
          path: |
            auth_state.json
            comments_log.csv
            complaints_log.csv
            daily_report_log.csv
            roi_map.json
          key: scraper-state-${{ runner.os }}-v1-${{ github.run_id }}
